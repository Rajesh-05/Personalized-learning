{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = \"\"\n",
    "\n",
    "class GradeTopic(BaseModel):\n",
    "    binary_score: str = Field(description=\"Whether it is a short topic, 'yes' or 'no'\")\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    Topic: str\n",
    "    character: str\n",
    "    chat_history: List[dict]\n",
    "    response: str\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "structured_llm_topic_grader = llm.with_structured_output(GradeTopic)\n",
    "\n",
    "# system = \"\"\"You are a teacher assessing whether the topic given by the user can be taught in a single conversation flow by a teacher to a student assuming that they know the prerequisites i.e. its a small topic that can taught clearly within 30 mins . \\n\n",
    "#     It does not need to be a stringent test. The goal is to extract the topic that the user want to learn from his message and figure out whether this topic can be taught efficiently within a shorter time frame. \\n\n",
    "#     If the topic can be taught in a short period of time , mark it as a short topic. \\n\n",
    "#     Give a binary score 'yes' or 'no' score to indicate whether the topic can be taught in a short amount of time .\"\"\"\n",
    "\n",
    "system = \"\"\"You are a teacher tasked with evaluating whether the topic mentioned by the user can be effectively taught in a single conversation flow, assuming the student knows the prerequisites. First, extract the specific topic from the user's query. Then, assess whether the topic can be introduced and explained at a high level within 20 minutes, covering the key concepts and main ideas without delving into detailed technical aspects.\n",
    "\n",
    "If the topic can be taught at this high-level overview within 20 minutes, mark it as a short topic.\n",
    "\n",
    "Provide a binary 'yes' or 'no' score to indicate whether the topic can be taught within 20 minutes.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User : {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "topic_grader = prompt | structured_llm_topic_grader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade = topic_grader.invoke(\"hey chat gpt can you teach me bagging\")\n",
    "grade.binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LessonPlan(BaseModel):\n",
    "    plan: List[str] = Field(description=\"Returns a lesson plan as list of strings \")\n",
    "\n",
    "structured_lesson_planner = llm.with_structured_output(LessonPlan)\n",
    "\n",
    "system_lp = \"\"\"Now, given the topic '[Insert Topic Here],' generate a list of key subtopics that should be covered. Provide the output as a plain list of topics, with no additional text or explanations.\"\"\"\n",
    "\n",
    "prompt_lp = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_lp),\n",
    "        (\"human\", \"User : {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lesson_planner = prompt_lp | structured_lesson_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LessonPlan(plan=['Introduction to bagging', 'Types of bagging', 'Advantages of bagging', 'Disadvantages of bagging', 'Applications of bagging', 'Example of bagging in machine learning', 'Comparison of bagging with other ensemble methods'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesson_planner.invoke(\"bagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicRecommend(BaseModel):\n",
    "    suggest: List[str] = Field(description=\"Returns recommendation topics as list of strings \")\n",
    "\n",
    "structured_topics_suggest = llm.with_structured_output(TopicRecommend)\n",
    "\n",
    "system_ts = \"\"\"Extract the main topic from the user's query. Based on the extracted topic, provide a list of five related subtopics that are distinct from the main topic and can be learned within a short span of time (around 20 minutes). Make sure to focus on smaller, digestible concepts that fit within this time frame. Do not include the main topic itself in the output.\"\"\"\n",
    "\n",
    "prompt_ts = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_ts),\n",
    "        (\"human\", \"User : {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "topics_suggester = prompt_ts | structured_lesson_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is Random Forest?',\n",
       " 'Types of Decision Trees',\n",
       " 'How Random Forest Works',\n",
       " 'Advantages of Random Forest',\n",
       " 'Common Applications of Random Forest']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_suggester.invoke(\"Random Forest\").plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_func(query):\n",
    "    state = topic_grader.invoke(query).binary_score\n",
    "    print(state)\n",
    "    if state == \"yes\":\n",
    "        lp = lesson_planner.invoke(query).plan\n",
    "        lp_str = \"Topics to Explore and Delve Into : \" \n",
    "        # for i in lp : \n",
    "        #     lp_str += \"--  \" + i + \"\\n\"\n",
    "        return {\n",
    "            \"status\":\"Success\",\n",
    "            \"plan\" : lp ,\n",
    "            \"plan_str\" : lp_str\n",
    "        }\n",
    "    else:\n",
    "        sp = topics_suggester.invoke(query).plan\n",
    "        sp_str = \"Select a Subtopic to Dive Deeper : \"\n",
    "        # for i in sp :\n",
    "        #     sp_str += \"-- \" + i + \"\\n\"\n",
    "        return {\n",
    "            \"status\" : \"Failure\",\n",
    "            \"plan\" : sp ,\n",
    "            \"plan_str\" : sp_str\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "res = topic_func(\"ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Failure',\n",
       " 'plan': ['Machine Learning',\n",
       "  'Supervised Learning',\n",
       "  'Unsupervised Learning',\n",
       "  'Overfitting and Underfitting',\n",
       "  'Gradient Descent'],\n",
       " 'plan_str': 'Select a Subtopic to Dive Deeper : '}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x251833a4c40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify , make_response , send_file\n",
    "import re \n",
    "import ast\n",
    "from flask_cors import CORS\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate , ChatPromptTemplate\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/select_topic', methods=['POST'])\n",
    "def select_topic() :\n",
    "    data = request.json  # Get JSON data from the request\n",
    "    topic = data.get('topic') \n",
    "    res = topic_func(topic)\n",
    "    return jsonify(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"you help students in learning efficiently by guiding discussions on specific topics and encouraging meaningful conversation. When provided with a topic and past conversation, share your thoughts to keep the discussion focused. Additionally, engage students by asking relevant Multiple-Choice Questions (MCQs) related to the topic. Present each MCQ in a structured JSON format as follows: {\n",
    "  \"question\": \"What is the capital of France?\",\n",
    "  \"option_A\": \"A) Paris\", \"explanation_A\": \"Correct, Paris is the capital of France.\" ,\n",
    "  \"option_B\": \"B) London\", \"explanatio_B\": \"Incorrect, London is the capital of the United Kingdom.\" ,\n",
    "  \"option_C\": \"C) Berlin\", \"explanation_C\": \"Incorrect, Berlin is the capital of Germany.\" ,\n",
    "  \"option_D\": \"D) Madrid\", \"explanation_D\": \"Incorrect, Madrid is the capital of Spain.\" ,\n",
    "  \"correct\" : \"A\"\n",
    "}\n",
    "Always stay on topic, avoid revealing personal information, and point to a specific student to encourage participation Impotant note Is that you need to ask questions only when I instruct you to ask else just indulge in the conversation and above prompt is just the format and info not the instruction dont use any extra messages  while or after asking questions\n",
    "NOTE : Scrictly Do not include any other content or explanation while asking question , follow the above example \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = 'llama-3.1-8b-instant'\n",
    "memory=ConversationBufferWindowMemory(k=5)\n",
    "groq_chat = ChatGroq(\n",
    "            groq_api_key=os.environ['GROQ_API_KEY'], \n",
    "            model_name=model\n",
    "    )\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an intelligent tutor named Zen, embedded within an application called SenseAI. Your role is to assist students in learning efficiently through your conversations. When provided with a specific topic and past conversation from a discussion room, you should share your thoughts on that particular topic, guiding students back to the subject of discussion. Always stay focused on the topic, encouraging students to engage in meaningful conversation around it. Avoid revealing any personal information or deviating from the subject matter.\"),\n",
    "        # MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"message history :{history} current input : {user_input}\")\n",
    "    ]\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "            llm=groq_chat,\n",
    "            memory=memory,\n",
    "            prompt = prompt,\n",
    "            input_key = \"user_input\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_summary = {}\n",
    "chat_history = []\n",
    "curr_topic = \"hello\"\n",
    "\n",
    "\n",
    "system_convo = \"\"\"\n",
    "You are an experienced and engaging teacher, dedicated to teaching the user based on the given topic and chat history. Your responses should feel human-like, natural, and conversational, with a focus on keeping the conversation concise.\n",
    "\n",
    "1. Explain concepts clearly and concisely, using examples and analogies where appropriate, but always aim for brevity. Assume the user has basic knowledge of the topic but needs to learn the details.\n",
    "2. Keep responses short and direct. If a more detailed explanation is needed, break it down into bullet points, with each point as a single string in a list.\n",
    "3. Ask questions related to the content you just taught or pose simple questions that reinforce the user's understanding of the topic.\n",
    "4. If the user's response is correct, acknowledge it and provide a brief follow-up. If incorrect or incomplete, gently correct them and offer a short, clear explanation.\n",
    "5. *Always format your output as a list of strings*. If the response is short, it should be a list containing a single string. For longer explanations, structure the information into a list where each element is a string.\n",
    "6. Ensure the conversation stays on topic. If the user diverges from the topic, gently guide them back to the subject at hand.\n",
    "7. Your goal is to ensure that the user learns the topic effectively while maintaining an engaging, brief, and interactive conversation.\n",
    "\n",
    "*Remember, regardless of the response length, always return the output as a list of strings.*\n",
    "convert the response into a list of strings at the end\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_convo = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_convo),\n",
    "        (\"human\", \"Topic: {topic}\\n\\nChat History: {chat_history}\\n\\nUser: {input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a responder pipeline that combines the prompt, language model, and output parser\n",
    "responser = prompt_convo | structured_lesson_planner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/chat', methods=['POST'])\n",
    "def chat() :\n",
    "    data = request.json  # Get JSON data from the request\n",
    "    topic = data.get('topic')  # Extract the topic from the data\n",
    "    query = data.get('query') \n",
    "    return jsonify(convo(topic, query))\n",
    "def convo(topic, query):\n",
    "    global curr_topic\n",
    "    try:\n",
    "        if topic.lower() != curr_topic.lower():\n",
    "            if chat_history:\n",
    "                global_summary[curr_topic] = chat_history  # Save the chat history for the previous topic\n",
    "            curr_topic = topic\n",
    "            chat_history.clear()  # Clear chat history for the new topic\n",
    "        \n",
    "        \n",
    "        # Prepare the context for the LLM\n",
    "        context = {\n",
    "            \"topic\": topic,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"input\": query\n",
    "        }\n",
    "        \n",
    "        # Generate the response\n",
    "        response = responser.invoke(context)\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        chat_history.append({\"role\": \"system\", \"content\": response})\n",
    "        print(response.plan)\n",
    "        return {\n",
    "            \"status\" : \"Success\",\n",
    "            \"response\" : response.plan\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # General exception handling\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        convo(topic,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/mcq', methods=['POST'])\n",
    "def mcq() :\n",
    "    try :\n",
    "        data = request.json\n",
    "        topic = data.get(\"topic\", \"\")\n",
    "        print(topic)\n",
    "        response = conversation.run({\"user_input\": instructions + topic })\n",
    "        res = json.loads(response)\n",
    "        return jsonify(res)\n",
    "    except :\n",
    "        mcq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5005\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Sep/2024 14:17:57] \"POST /select_topic HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-24 14:18:10,514] ERROR in app: Exception on /select_topic [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\python\\lib\\site-packages\\flask_cors\\extension.py\", line 176, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_16780\\1268028132.py\", line 5, in select_topic\n",
      "    res = topic_func(topic)\n",
      "  File \"C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_16780\\1361611603.py\", line 5, in topic_func\n",
      "    lp = lesson_planner.invoke(query).plan\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2873, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5055, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 265, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 698, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 555, in generate\n",
      "    raise e\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 545, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 770, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 472, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 178, in create\n",
      "    return self._post(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 1194, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 896, in request\n",
      "    return self._request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 987, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{\"plan\": [\"Supervised Learning Overview\", \"Types of Supervised Learning\", \"Linear Regression\", \"Decision Trees\", \"Random Forests\", \"Support Vector Machines\", \"Neural Networks\", \"Evaluation Metrics\", \"Hyperparameter Tuning\", \"Model Selection\"]}'}}\n",
      "127.0.0.1 - - [24/Sep/2024 14:18:10] \"POST /select_topic HTTP/1.1\" 500 -\n",
      "[2024-09-24 14:28:20,081] ERROR in app: Exception on /select_topic [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_transports\\default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n",
      "    stream = self._connect(request)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 122, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_backends\\sync.py\", line 205, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"c:\\python\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 925, in _request\n",
      "    response = self._client.send(\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_transports\\default.py\", line 232, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"c:\\python\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\python\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\python\\lib\\site-packages\\flask_cors\\extension.py\", line 176, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\python\\lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_16780\\1268028132.py\", line 5, in select_topic\n",
      "    res = topic_func(topic)\n",
      "  File \"C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_16780\\1361611603.py\", line 2, in topic_func\n",
      "    state = topic_grader.invoke(query).binary_score\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2873, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5055, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 265, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 698, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 555, in generate\n",
      "    raise e\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 545, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 770, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\python\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 472, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 178, in create\n",
      "    return self._post(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 1194, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 896, in request\n",
      "    return self._request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 949, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 1020, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 949, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 1020, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\python\\lib\\site-packages\\groq\\_base_client.py\", line 959, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "groq.APIConnectionError: Connection error.\n",
      "127.0.0.1 - - [24/Sep/2024 14:28:20] \"POST /select_topic HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=5005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversation.run({\"user_input\": instructions + \"ask a question as on template on Overfitting in ML\"  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "s = json.loads(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
